{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will curate a math dataset avaliable on huggingface: https://huggingface.co/datasets/ajibawa-2023/Maths-College"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset to a local file\n",
    "import datasets as ds\n",
    "\n",
    "# Load the dataset\n",
    "data = ds.load_dataset(\"ajibawa-2023/Maths-College\")\n",
    "data.save_to_disk(\"MathDataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load the dataset and make some small changes to the instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets as ds\n",
    "dts = ds.load_from_disk(\"MathDataset\")\n",
    "def edit_instruction(example):\n",
    "    old_instruction = \"Write an educational piece suited for college students related to the following text snippet:\"\n",
    "    new_instruction = \"Write an educational piece related to the following text snippet:\"\n",
    "    example['context'] = example['instruction'].replace(old_instruction, '')\n",
    "    # remove starting \\n from the context if it exists\n",
    "    if example['context'].startswith('\\n'):\n",
    "        example['context'] = example['context'][1:]\n",
    "    example['instruction'] = new_instruction\n",
    "    return example\n",
    "new_dts = dts.map(edit_instruction)\n",
    "new_dts.save_to_disk(\"MathInstructDataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet, now we have a dataset in the correct format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This edited dataset is now on huggingface [Math-instruct-dataset](https://huggingface.co/datasets/patrickjmcbride/math-instruct-dataset)\n",
    "\n",
    "It has a 75/25 train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = ds.load_dataset(\"patrickjmcbride/math-instruct-dataset\")\n",
    "data.save_to_disk(\"MathInstructDataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now were going create a text column that is the question, context, and answer combined into one string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets as ds\n",
    "\n",
    "all_data = ds.data = ds.load_dataset(\"patrickjmcbride/math-instruct-dataset\")\n",
    "\n",
    "def generate_full_text(entry):\n",
    "    return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{entry[\"instruction\"]}\\n\\n### Input:\\n{entry[\"context\"]}\\n\\n### Response:\\n\"{entry[\"output\"]}\"\"\"\n",
    "# For each example in test and train, generate the full text and add it to the dataset under the key \"text\"\n",
    "for split in [\"train\", \"test\"]:\n",
    "    all_data[split] = all_data[split].map(lambda x: {\"text\": generate_full_text(x)})\n",
    "\n",
    "# Save the dataset\n",
    "all_data.save_to_disk(\"MathInstructDataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = ds.load_from_disk(\"MathInstructDataset\")\n",
    "# print the column names\n",
    "print(data.column_names)\n",
    "print(\"Test: \", len(data[\"test\"]))\n",
    "print(\"Train: \", len(data[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "%env HF_TOKEN=api_token\n",
    "data = ds.load_from_disk(\"MathInstructDataset\")\n",
    "# push the dataset to the hub\n",
    "data.push_to_hub(\"math-instruct-dataset\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to get an idea of how many tokens are in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "from datasets import load_from_disk, concatenate_datasets\n",
    "\n",
    "# Load the datasets\n",
    "data = load_from_disk(\"MathInstructDataset\")\n",
    "\n",
    "# Concatenate the datasets\n",
    "train_data = data[\"train\"]\n",
    "test_data = data[\"test\"]\n",
    "\n",
    "# Create a new concatenated dataset\n",
    "concatenated_data = concatenate_datasets([train_data, test_data])\n",
    "\n",
    "# Print the length of the new concatenated dataset\n",
    "print(len(concatenated_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import concatenate_datasets, load_from_disk\n",
    "\n",
    "# Load the datasets\n",
    "data = load_from_disk(\"MathInstructDataset\")\n",
    "\n",
    "# Concatenate the datasets\n",
    "train_data = data[\"train\"]\n",
    "test_data = data[\"test\"]\n",
    "concatenated_data = concatenate_datasets([train_data, test_data])\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"failspy/Meta-Llama-3-8B-Instruct-abliterated-v3\")\n",
    "\n",
    "# Function to tokenize a single example\n",
    "def tokenize_example(example):\n",
    "    return len(tokenizer(example[\"text\"])[\"input_ids\"])\n",
    "\n",
    "# Tokenize using multiple threads\n",
    "num_threads = 12  # Adjust the number of threads based on your CPU\n",
    "dic_sizes = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    results = list(executor.map(tokenize_example, concatenated_data))\n",
    "\n",
    "# Print the result\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(results, bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "count_less_than_1024 = 0\n",
    "for i in results:\n",
    "    if i < 1536:\n",
    "        if i < 1024:\n",
    "            count_less_than_1024 += 1\n",
    "        else:\n",
    "            count += 1\n",
    "print(count)\n",
    "# print percentage of examples that are more than 1024 tokens and less than 1536 tokens\n",
    "print(\"percent in range 1024-1536: \",count / len(results) * 100)\n",
    "print(\"percent less than 1024:     \",count_less_than_1024 / len(results) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that we can create 2 datasets, one with a max token length of 1024 and one with a max token length of 1536 and min token length of 1024.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "data = load_from_disk(\"MathInstructDataset\")\n",
    "\n",
    "# Concatenate the datasets\n",
    "train_data = data[\"train\"]\n",
    "test_data = data[\"test\"]\n",
    "concatenated_data = concatenate_datasets([train_data, test_data])\n",
    "\n",
    "# save the concatenated dataset\n",
    "concatenated_data.save_to_disk(\"MathInstructDataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a new dataset with 3 splits, small, medium, and large\n",
    "\n",
    "We will use the following token lengths for each split:\n",
    "- small: [min-1024)\n",
    "- medium: [1024-1536)\n",
    "- large: [1536, max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import concatenate_datasets, load_from_disk\n",
    "\n",
    "# Load the datasets\n",
    "data = load_from_disk(\"MathInstructDataset\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "\n",
    "# Function to tokenize a single example\n",
    "def tokenize_example(example):\n",
    "    return len(tokenizer(example[\"text\"])[\"input_ids\"])\n",
    "\n",
    "# Tokenize using multiple threads\n",
    "num_threads = 16  # Adjust the number of threads based on your CPU\n",
    "dic_sizes = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    results = list(executor.map(tokenize_example, data))\n",
    "\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "small_data = {\"instruction\": [], \"context\": [], \"output\": [], \"text\": []}\n",
    "medium_data = {\"instruction\": [], \"context\": [], \"output\": [], \"text\": []}\n",
    "large_data = {\"instruction\": [], \"context\": [], \"output\": [], \"text\": []}\n",
    "\n",
    "ct = 0\n",
    "for example in data:\n",
    "    # Get the number of tokens\n",
    "    num_tokens = results[ct]\n",
    "    \n",
    "    # Add the example to the corresponding split\n",
    "    if num_tokens < 1024:\n",
    "        for key in small_data.keys():\n",
    "            small_data[key].append(example[key])\n",
    "    elif num_tokens < 1536:\n",
    "        for key in medium_data.keys():\n",
    "            medium_data[key].append(example[key])\n",
    "    else:\n",
    "        for key in large_data.keys():\n",
    "            large_data[key].append(example[key])\n",
    "    ct += 1\n",
    "\n",
    "import json\n",
    "with open(\"small_data.json\", \"w\") as f:\n",
    "    json.dump(small_data, f)\n",
    "with open(\"medium_data.json\", \"w\") as f:\n",
    "    json.dump(medium_data, f)\n",
    "with open(\"large_data.json\", \"w\") as f:\n",
    "    json.dump(large_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets as ds\n",
    "import json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are done one at a time since it requires a lot of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take about 15GB of ram to run\n",
    "with open(\"small_data.json\", \"r\") as f:\n",
    "    small_data = json.load(f)\n",
    "\n",
    "small_dataset = ds.Dataset.from_dict(small_data)\n",
    "small_dataset.save_to_disk(\"MathInstructSmall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take about 10GB of ram to run\n",
    "with open(\"medium_data.json\", \"r\") as f:\n",
    "    medium_data = json.load(f)\n",
    "\n",
    "medium_dataset = ds.Dataset.from_dict(medium_data)\n",
    "medium_dataset.save_to_disk(\"MathInstructMedium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take about 70GB of ram to run\n",
    "with open(\"large_data.json\", \"r\") as f:\n",
    "    large_data = json.load(f)\n",
    "\n",
    "large_dataset = ds.Dataset.from_dict(large_data)\n",
    "large_dataset.save_to_disk(\"MathInstructLarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a new dataset with 3 splits, small, medium, and large.\n",
    "\n",
    "This will not take a lot of memory since the splits are already created as separate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets as ds\n",
    "small = ds.load_from_disk(\"MathInstructSmall\")\n",
    "medium = ds.load_from_disk(\"MathInstructMedium\")\n",
    "large = ds.load_from_disk(\"MathInstructLarge\")\n",
    "\n",
    "data_dict = ds.DatasetDict({\"small\": small, \"medium\": medium, \"large\": large})\n",
    "\n",
    "data_dict.save_to_disk(\"MathInstructBinned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will push the datasets to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_data = ds.load_from_disk(\"MathInstructBinned\")\n",
    "%env HF_TOKEN=api_token\n",
    "binned_data.push_to_hub(\"math-instruct-binned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets as ds\n",
    "binned_data = ds.load_dataset(\"patrickjmcbride/math-instruct-binned\")\n",
    "# get the length of each split\n",
    "len_small = len(binned_data[\"small\"])\n",
    "len_medium = len(binned_data[\"medium\"])\n",
    "len_large = len(binned_data[\"large\"])\n",
    "\n",
    "# print the percentage of each split\n",
    "print(\"Small: \", len_small / (len_small + len_medium + len_large) * 100)\n",
    "print(\"Medium: \", len_medium / (len_small + len_medium + len_large) * 100)\n",
    "print(\"Large: \", len_large / (len_small + len_medium + len_large) * 100)\n",
    "\n",
    "binned_data.save_to_disk(\"MathInstructBinned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of examples in each split\n",
    "print(\"Small: \", len_small)\n",
    "print(\"Medium: \", len_medium)\n",
    "print(\"Large: \", len_large)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
